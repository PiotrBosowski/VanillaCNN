X WSZYSTKIE neurony tworzone podczas kompilacji sieci, nie podczas deklarowania warstw

X connectowanie się warstw

X uproszczenie modelu - wykluczenie warstwy "Connect"

X uproscic strukture klas neuronow

X przerobić łączenie się warstw:
	albo warstwy łączą się same -> dodanie nowego typu warstwy wiąże się ze zmianą kodu innych warstw
	albo łączenie się nadzoruje ConnectingEngine -> dodanie nowego typu warstwy wiąże się ze zmianą kodu connectingEngine

	nowy pomysł: każda klasa ma skojarzoną klasę ...Connector, która odpowiada za łączenie z poprzednią warstwą
	vector i matrix dziedziczą ze wspólnej klasy "container" np.

X Możliwe połączenia:
	FullyConnectedLayer:
		FullyconnectedLayer
		InputLayer
		ConvolutionLayer
		DownsamplingLayer

	OutputLayer:
		FullyConnectedLayer
		InputLayer
		ConvolutionLayer
		DownsamplingLayer

	ConvolutionLayer:
		InputLayer
		ConvolutionLayer
		DownsamplingLayer

	DownsamplingLayer:
		ConvolutionLayer
		DownsamplingLayer
		InputLayer

	InputLayer:
		-

X Możliwe rodzaje połączeń kontenerowych:
	1:1 (np. convolution->downsampling)
	1:all (np. fullconnected->convolution)
	1:random (druga downsampling->convolution)

X Możliwe rodzaje połączeń neuronowych:
	1:area (np. convolution->downsampling)
	1:all

//Rename proposeConnections -> attach

X Oddzielić funkcjonalność drukowania outputu od modelu

X zrobić custom exceptions

X dokończyć neuronsConnectingStrategies

X przerobić warstwy na propotypy
  InputLayer - prototyp
  _InputLayer - faktyczna warstwa

  wywalic exception handling z maina, funkcje wywolywane w mainie maja byc nothrow

X zrobić optymalne łączenie sie warstw

dorobić containers-> 1toRandom

X posprzątać w konstruktorach warstw

	mozliwosc zapisu modelu do pliku

	add network constructor

	na teraz:
X	wywalic klasy neuronowe, zostawic tylko neuron (wlasciwie to zostawic)
X	dodac wagi lub referencje wag do klasy connection/weighted connection

X	containers:
X	weighted/unweightedvector
X	weighted/unweightedmatrix

X ogarnąć gita troche lepiej zeby sie nie wyjabec z rowerka XD

add RawLayer so the users can create their own layers
scalić neuron factory i neuron connecting factory
scalić container factory i container connecting factory

doxygen

containers i container - kompozyt

kazda warstwa musi byc IConnectcible

Neuron nie ma referencji do poprzedniego neurona, tylko do jego value TODODODOO!!!

poruszanie się po wektorach - iteratory zamiast operator[]

rationale:
Jakiś czas temu zacząłem pisać mały framework do tworzenia sieci neuronowych ANN/CNN poszerzony o możliwość tworzenia wlasnych typów sieci/połączeń/neuronów; robiłem to głównie po to, żeby sprawdzić, czy rzeczywiście rozumiem jak działają sieci neuronowe + nauczyć się nieco modern C++, poużywać wzorców projektowych i poprzestrzegać zasad dobrego programowania obiektowo-orientowanego z mnniejszą lub większą skutecznością. Takich neuronowych frameworków istnieją już dziesiątki, więc nie było potencjału na zaistnienie na rynku ani nic takiego. Jestem +/- w połowie, mam stabilnie 'kompilującą' się sieć, która tworzy poprawne połączenia, mam napisane testy, wszystko do tej pory działa jak należy. W tej chwili przyszła pora na napisanie algorytmów pracy tej sieci, czyli algorytmu propagacji (w skrócie: algorytm odpowiedzialny za przetwarzanie danych wejściowych od warstwy pierwszej do warstwy ostatniej) i propagacji wstecznej (algorytm odpowiedzialny za dostosowywanie wag połączeń począwszy od warstwy ostatniej do warstwy pierwszej - istota sieci neuronowych). Są to naprawdę proste rzeczy, jedynie brzmią skomplikowanie.
Tutaj wjeżdża tytułowa CUDA. Żeby zrobić to jako tako wydajnie, trzeba jak najbardziej urównoleglić obliczenia. Wiesz jak to jest w CUDZIE - wysyłasz sobie potężną tablicę 1024*1024 do GPU i elo, zaraz masz policzone. Mój problem polega na tym, że sieć neuronowa - w przeciwieństwie do tablicy - nie jest jednolitym obszarem pamięci, który łatwo wysłać, wręcz przeciwnie: pełno w niej wskaźników, referencji (połączenie neuronów to nic innego jak właśnie wskaźnik) rozsianych po całym RAMie, a sięganie przez te wskaźniki i referencje do faktycznych wartości, które należy wysłać do GPU, zjada 99,5% czasu CPU. Wstępne próby mówią mi, że rozwiązanie, które próbuję zastosować, będzie po prostu chujowe - procesor będzie przygotowywał dane dla GPU przez 100ms, a GPU będzie zwracać wynik w 0.1 ms. Kompletnie bez sensu.
Jedynym rozwiązaniem jest wczytanie całej sieci neuronowej do GPU przy starcie programu. Idealna sytuacja, nie tracisz w ogóle zasobów na kopiowanie CPU<->GPU, które zawsze jest wąskim gardłem. Żeby jednak łatwo skopiować sieć, trzeba utworzyć ją tak, aby w pamięci zajmowała N kolejnych bajtów. To jest mega ważne: tworząc warstwy, tablice neuronów itd. nie możesz korzystać z operatora new, bo tracisz wpływ na to, gdzie kompilator zaalokuje pamięć na dany neuron. Nie możesz korzystać z std::vector, bo vector alokuje elementy dynamicznie. Nie możesz korzystać ze smart pointerów, bo one alokują pamięć dynamicznie. Wszystko musi być alokowane jako obiekt lokalny. Jeśli tworzysz tablicę neuronów, nie możesz napisać Neuron* table = new Neuron[N];, musisz napisać: Neuron table[N];. Różnica jest zajebiście wielka - w drugim przypadku N musi być znane już podczas kompilacji. Musisz znać wszystkie liczby opisujące sieć neuronową w momencie kompilacji.

TLDR: trzeba przerobic wszystko, aby neural network byla local objectem